---
title: "Biopsy Biomarker Variable Selection"
author: "Zehui Bai"
date: 'Stand: `r format(Sys.time(), "%F %H:%M Uhr")`'
output:
  md_document:
    toc: yes
  pdf_document:
    toc: yes
  html_document:
    df_print: paged
    number_sections: no
    toc: yes
    toc_float: yes
fontsize: 10pt
editor_options:
  chunk_output_type: console
colorlinks: yes
---

```{r setup, include=FALSE, echo = FALSE,message = FALSE, error = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE)

# load package
pkgs = c("ggplot2", "ggcorrplot", "car", "reshape2", "papeR", "Hmisc", "GGally", "bestglm", "leaps", "class", "kknn", "e1071", "caret", "glmnet", "psych", "dplyr", "earth", "MASS", "knitr", "kableExtra", "randomForest", "InformationValue")
inst = lapply(pkgs, library, character.only = TRUE) 
```


# Task description

For 569 patients with benign or malignant breast cancer a biopsy was taken (DataScientist_test_data.csv). Based on that biomarkers (features) were computed. 

The objective of this task is to **derive a cutoff of one of the features** and to **find a combination of potentially diagnostic variables** for the identification of benign vs. malignant tumors.

Specifically, complete the following exercises

1) Provide a descriptive overview of the data.
2) Analyze the correlation structure of the biomarkers.
3) Identify potential biomarker candidates related to the cancer diagnosis (benign vs. malignant).
4) Evaluate the performance of the model with the selected biomarkers.
5) Find a cutoff for the mean area separating the patients into benign and malignant breast cancer. In case you are not familiar with cutoff detection, please try to collect ideas how this could be done.


# Data Exploration

First of all, the total display variable structure and type, and whether it is missing. 
Where Diagnosis is a target variable, the rest is a biomarker, and there is no missing value.

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
# import dataset
data <- read.csv("~/Library/Mobile Documents/com~apple~CloudDocs/02. Programm/01 R/00 Data/Biopsy.csv")
## Drop the patient ID and blank column
biopsy <- data[-c(1,33)]
str(biopsy)

## Explore data: Check the missing ratio of biomaker
sapply(biopsy,function(x)sum(is.na(x)))
```

## Explore the variable diagnosis

For variable diagnosis, "B" denotes the benign tumor and "M" denotes the malignant tumor. 
First check the percent of the two categories of variable diagnosis and visualizate.
It is very important to check the ratio of benign and malignant in the response variable diagnosis 
to ensure that the data is divided and balanced. If a result is too sparse, it may cause problems of mis-classification. After checking the ratio of benign to malignant is more ideal, about 5:3.

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
biopsy$diagnosis <- factor(biopsy$diagnosis, 
                           levels = c("B", "M"), 
                           labels = c("Benign", "Malignant"))
table(biopsy$diagnosis)
## Visualization
theme_set(theme_bw())
ggplot(data = biopsy) + 
  geom_bar(mapping = aes(x = diagnosis, y = ..prop..,  group=1),width=.5)+
  labs(title="Breast Cancer Classification Visualization", 
       subtitle="Relative Proportions")
```


## Explore the variables of biomarkers

As indicated, therefore, all biomarkers are numerical variables, and thus exhibit results in a descriptive statistical manner.

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
## Explore data: Summary tables for numerical variables
## HTML Output
biopsy_DS <- papeR::summarize(biopsy, type = "numeric", group = "diagnosis")
rownames(biopsy_DS) <- NULL
kable(biopsy_DS, caption = "Descriptive statistics of biomarkers", format = "html") %>% 
  kable_styling(latex_options = "striped")

biopsy.m <- melt(biopsy, id.var = "diagnosis")
ggplot(data = biopsy.m, aes(x = diagnosis, y = value)) + 
  geom_boxplot() + facet_wrap(~ variable, ncol = 5) +
  labs(title="Breast Cancer Biomarkers Visualization", 
       subtitle="Using Boxplot")+
  theme(legend.position="top")

par(mar=c(1,1,1,1))
hist.data.frame(biopsy[,c(2:16)], n.unique=1, 
                mtitl = "Biomarkers Visualization using Histogram 1")
hist.data.frame(biopsy[,c(17:31)], n.unique=1, 
                mtitl = "Biomarkers Visualization using Histogram 2")
```
	

## Feature scaling using z-score

```{r,echo = F,message = FALSE, error = FALSE, warning = FALSE}
biopsy_ZS <- biopsy
biopsy_ZS[-1] <- as.data.frame(scale(biopsy[-1]))

## Visualization of the new results
biopsy_ZS.m <- melt(biopsy_ZS, id.var = "diagnosis")
ggplot(data = biopsy_ZS.m, aes(x = diagnosis, y = value)) + 
  geom_boxplot() + facet_wrap(~ variable, ncol = 5) + 
  labs(title="Breast Cancer Biomarkers Visualization", 
       subtitle="Using Boxplot after z-score transformation") +
  theme(legend.position="top")

ggplot(data = biopsy_ZS.m, aes(x = value)) + 
  geom_histogram(aes(color = diagnosis, fill = diagnosis),
                 binwidth = 0.3,
                 alpha=0.5,
                 position = "identity") + 
  facet_wrap(~ variable, ncol = 5) +
  labs(title="Breast Cancer Biomarkers Visualization", 
       subtitle="Using Histogram after z-score transformation") +
  theme(legend.position="top")
```

**Note**: Other conversion (normalization) can also be considered, as follows, the values of all biomarkers fall in the interval [0, 1], this document only uses z-score conversion.

```
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
biopsy_NM <- biopsy
biopsy_NM[-1]<- as.data.frame(lapply(biopsy[-1], normalize)) 

```


## Summary of the data exploration

* No missing values.
* Except for the variables fractal_dimension_mean, texture_se, smoothness_se, symmetry_se, other biomarkers differ significantly between diagnoses.
* As shown in Boxplot Histogram, the measurement scale of some biomarkers is very large, such as area_mean, area_worst, which may cause problems in the classification algorithm, so all variables will be scaled to the standard value using z-score transformation.
* Most histograms are distributed asymmetrically, similar to exponential distributions.






# Correlation Exploration

## Correlation structure

```{r,echo = F,message = FALSE, error = FALSE, warning = FALSE}
## Compute correlation matrix
## round(cor(biopsy[-1]), 2)

## Correlation greater than 0.9
CorrMatrixtable <- function(cormat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut]
  )
}

## cormat : matrix of the correlation coefficients
Corr_Results <- CorrMatrixtable(round(cor(biopsy_ZS[-1]), 3))
Corr_Results_out <- Corr_Results[Corr_Results$cor>0.9,]
rownames(Corr_Results_out) <- c(1:nrow(Corr_Results_out))

kable(Corr_Results_out, 
      caption = "Correlation of biomarkers greater than 0.9", format = "html") %>% 
  kable_styling(latex_options = "striped")

## Visualization: draw a correlogram
ggcorr(biopsy_ZS[-1], nbreaks=8, palette='PuOr', label=TRUE, label_size=4, size = 2) +
  ggtitle("Correlation Matrix of biomarkers") + 
  theme(plot.title = element_text(hjust = 0.5))
```


## Summary of the correlation exploration

The correlation between variables should not be ignored. 21 pairs of highly correlated biomarkers are found, with correlation coefficients greater than 0.9, which would lead to multicollinearity problems.


# Biomarker Candidates Identification and Modeling

## Idea

The target variable "diagnosis" is a binary variable. Therefore, in order to identify biomarker candidates related to the classification of benign and malignant cancers, different classification algorithms can be used for modeling. Here, the following variable screening/modeling methods are compared.

1. Scenario 1: Logistic regression using full model including all biomarkers.
2. Scenario 2: Logistic regression, delete highly correlated biomarker variables in the full model.
3. Scenario 3: Logistic regression using model selection (backward) to reduce the characteristics of biomarkers from the full model and from the model after removing high-correlation terms.
4. Scenario 4: Regularized logistic regression using lasso regularization
5. Scenario 5: Regularized logistic regression using elastic net regularization
6. Scenario 6: Multiple adaptive regression splines (MARS)


In addition, in order to provide a more comprehensive comparison (here is no clear biomarker identification), the following machine learning methods will be introduced.

1. Linear discriminant analysis (LDA)
2. Principal component analysis (PCA) 
3. Support Vector Machine (SVM)
4. K-Nearest Neighbors (KNN)
5. Random Forest (RF)


## Data Preparation

In order to evaluate the predictive ability of the model, the data set is divided into a training data set (70%) and a test data set (30%). The training data set is used to build the model, and the test data set is used to detect the predictive ability of the model.

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE, fig.height= 10, fig.width=7}
set.seed(123)                       
ind <- sample(2, nrow(biopsy_ZS), replace = TRUE, prob = c(0.7, 0.3))
## the training data set with 70% data
biopsy.zs.train <- biopsy_ZS[ind == 1, ] 
## the test data set with 30% data
biopsy.zs.test <- biopsy_ZS[ind == 2, ] 

trainY <- biopsy_ZS$diagnosis[ind==1]
testY <-  biopsy_ZS$diagnosis[ind==2]

## Recode the diagnosis as numeric Benign 0 or Malignant 1
y_train <- ifelse(biopsy.zs.train$diagnosis == "Malignant",1,0)
y_test <- ifelse(biopsy.zs.test$diagnosis == "Malignant",1,0)
```


## Modeling

### Scenario 1: Logistic regression (Full Model)

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
LR.full.fit <- glm(diagnosis ~ ., family = binomial, data = biopsy.zs.train)

## view the coefficients of each predictor and its p-value
summary(LR.full.fit)

## 95% confidence interval
confint(LR.full.fit)


## Calculate VIF statistics
vif(LR.full.fit)
```


**Finding:**

* The algorithm does not converge.
* It can be seen from the correlation coefficient and VIF statistics (much greater than 5) that there will be a problem of collinearity.
* All predictors are not significant, the coefficient is too large, and the confidence interval is too wide. The full model did not provide any meaningful results and will not be evaluated.





### Scenario 2: Logistic regression (removing highly correlated items)

In the previous exploration of the data, 21 pairs of biomarkers were found to have high correlations, greater than 0.9. In order to avoid the problem of collinearity caused by high correlation, the following variables are removed from the full model and the model is re-modeled using logistic regression.

*radius_mean, perimeter_mean, area_mean, concave.points_mean, radius_se,area_se, compactness_se, 
texture_worst, perimeter_worst, area_worst*


```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
## Fisrt check the correlation of biomarkers after removing
new_cor <- CorrMatrixtable(round(cor(biopsy_ZS[-c(1,2,4,5,9,12,15,17,23,24,25)]), 3))
new_cor[new_cor$cor>0.9,]

## Modeling
LR.cor.fit <- glm(diagnosis ~ ., family = binomial, data = biopsy.zs.train[-c(2,4,5,9,12,15,17,23,24,25)])

## view the coefficients of each predictor and its p-value
summary(LR.cor.fit)

## 95% confidence interval
confint(LR.cor.fit)

## Calculate VIF statistics
vif(LR.cor.fit)

## Performance on the test dataset
LR.cor.probs <- predict(LR.cor.fit, newdata = biopsy.zs.test, type = "response")

## Calculate the confusion matrix 
InformationValue::confusionMatrix(testY, LR.cor.probs)
```


**Finding:**

* The model converged and it is interpretable, the biomarker variables **texture_mean, smoothness_mean, compactness_mean, fractal_dimension_se, radius_worst, fractal_dimension_worst** are significant, and the confidence interval of the variables also does not include 0.
* However, other variables are not significant and the CIs include 0.
* The problem of multicollinearity is alleviated, but it still exists from the variance inflation factor.



### Scenario 3: Logistic regression (backward seletion)

In Scenario 1 and Scenario 2, all variables of the full model are not significant, and after removing highly correlated variables, most models are still not significant. Therefore, backward logistic regression is used here to select the best model for the two models. The selection criteria is AIC.

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
## Selection from the full model of Scenario 1 
backward.full <- stepAIC(LR.full.fit, direction = "backward", trace = FALSE)
backward.full$formula

LR.Bestfull.fit <- glm(backward.full$formula, family = binomial, 
                       data = biopsy.zs.train)
summary(LR.Bestfull.fit)


## Selection from the model of Scenario 2 
backward.cor <- stepAIC(LR.cor.fit, direction = "backward", trace = FALSE)
backward.cor$formula

LR.Bestcor.fit <- glm(backward.cor$formula, family = binomial, 
                       data = biopsy.zs.train)
summary(LR.Bestcor.fit)


## Plot the main effect 
beta.table <- data.frame(summary(LR.Bestcor.fit)$coef)
beta.table$variable <- row.names(beta.table)
beta.table <- mutate(beta.table, estimate = Estimate, 
                     se = Std..Error, 
                     low = Estimate - 2*se, 
                     high = Estimate + 2*se,
                     p = Pr...z..)  

ggplot(beta.table, aes(y=estimate, x=variable, ymin=low, ymax=high, color = (p < 0.05))) + 
  geom_pointrange() +  
  geom_hline(yintercept = 0, col="darkgrey", lty = 3, lwd=2) + 
  coord_flip() + ggtitle("Best backward subset using AIC main effect plot")

## Performance on the test dataset
LR.Bestcor.probs <- predict(LR.Bestcor.fit, newdata = biopsy.zs.test, type = "response")

## Calculate the confusion matrix 
InformationValue::confusionMatrix(testY, LR.Bestcor.probs)
```


**Finding:**

* For the Scenario 1 full model, the algorithm does not converge after the model selection, and all the variables are still insignificant.
* For the Scenario 2 model, the algorithm converges after the model is selected, and most of the variables are significant, and the confidence interval does not include 0. Model comparison is possible to verify whether the fitted model has good performance on the test data set.



### Scenario 4: Lasso regression 

For high-dimensional data, techniques such as best subsets and stepwise feature selection will cause unbearable time costs. If we use the best subset method, we need to test 2^30^ models on a breast cancer data set. For which regularization is a very convenient method. For the determined parameter lambda, the model only needs to be executed once using penalized regression, so the efficiency will be greatly improved. Regularization can limit the coefficients and even reduces them to 0 to reduce the variable characteristics. In addition, regularization can also handle high correlations between variables, to solve the problem of multicollinearity.

Two regularization techniques are used for modeling here, namely Lasso regression (Scenario 4) and Elastic net (Scenario 5). 

Lasso regression uses L1-norm, that is, to minimize $RSS + \lambda(\sum|\beta_j|)$ as following. This shrinkage penalty term can shrink the feature weight to 0 compared to ridge regression to reduce the variable.

$$\sum_{i=1}^{n}\left(y_{i}-\sum_{j} x_{i j} \beta_{j}\right)^{2}+\lambda \sum_{j=1}^{p}\left|\beta_{j}\right|$$
In order to better reduce the error, cross-validation is used here for lasso regression.


```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
## Lasso Regression
## 3-fold cross-validation
set.seed(317)
lasso.cv = cv.glmnet(x=as.matrix(biopsy.zs.train[-1]), y=y_train, 
                     family = "binomial",  nfolds = 3)

## The two vertical dashed lines in the figure represent the log(lambda) of 
## the minimum MSE (the dashed line on the left) and the log(lambda) of one standard error of the minimum distance.
plot(lasso.cv,main="MSE vs. the log(lambda)")

## minimum
lasso.cv$lambda.min  

## show the fitted model
lasso.fit <- glmnet(x=as.matrix(biopsy.zs.train[-1]), y=y_train, 
                    family = "binomial", 
                    alpha = 1, 
                    lambda =lasso.cv$lambda.min) 
coef(lasso.fit)

## Predict on the test dataset
lasso.probs <- predict(lasso.cv, newx=as.matrix(biopsy.zs.test[-1]), 
                       type = "response", 
                       s = "lambda.min")

## Calculate the confusion matrix 
InformationValue::confusionMatrix(testY, lasso.probs)
```

**Finding:**

* Model dimensions are significantly reduced.



### Scenario 5: Elastic net regularization

Elastic Net combines the advantages of Ridge regression and Lasso regression, using L1 and L2 norms as following

$${\displaystyle {\hat {\beta }}\equiv {\underset {\beta }{\operatorname {argmin} }}(\|y-X\beta \|^{2}+\lambda _{2}\|\beta \|^{2}+\lambda _{1}\|\beta \|_{1}).}.$$

For the parameter a, a = 0 represents the ridge regression penalty, and a = 1 represents the Lasso penalty. A grid is established here to find the best a and lambda. The final values used for the model were alpha = 0.2 and lambda = 0.02.


```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
## Elastic net regularization
grid <- expand.grid(.alpha = seq(0,1, by=.2), 
                    .lambda = seq(0.00, 0.2, by = 0.02))

## Leave-One-Out Cross-Validation 
## FInd the best alpha and lambda with smallest MAE       
control <- trainControl(method = "LOOCV") 
set.seed(701)                            
enet.train = train(diagnosis ~ ., data = biopsy.zs.train, 
                   method = "glmnet", 
                   family = "binomial", 
                   trControl = control, 
                   tuneGrid = grid)
enet.train


## Test in the test datasets with resulted alpha and lambda
enet.fit <- glmnet(x=as.matrix(biopsy.zs.train[-1]), y=y_train, 
                   family = "binomial",  alpha = 0.2, lambda = 0.02)
enet.coef <- coef(enet.fit, s = 0.02, exact = TRUE)
enet.coef

## Test in the test datasets
enet.probs <- predict(enet.fit, 
                      newx = as.matrix(biopsy.zs.test[-1]), 
                      type = "response",  s= 0.02)

## Calculate the confusion matrix 
InformationValue::confusionMatrix(testY, enet.probs)
```


**Finding:**

* The variables were not reduced like Lasso regression, and 6 biomarkers were eliminated.




### Scenario 6: Multiple adaptive regression splines (MARS)

There are three main methods for using probability-based linear models to predict response variables:

1. Logistic regression
2. Linear discriminant analysis
3. Multivariate adaptive regression spline 

MARS automatically creates a piecewise linear model, which can intuitively step into nonlinearity. For classification problems, multiple adaptive regression splines can flexibly build linear models and nonlinear models. Therefore, MARS can be used here to solve the complicated relationship between biomarkers.

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
## Use 5-fold cross-validation to select the model
set.seed(132)
MARS.fit <-  earth(diagnosis ~ ., data = biopsy.zs.train,
                   pmethod = "cv",
                   nfold = 5,
                   ncross = 3,
                   degree = 1,
                   minspan = -1,
                   glm=list(family=binomial))
summary(MARS.fit)

## The following variables are used to build the MARS model
name <- evimp(MARS.fit)
rownames(name)

## Prediction on the test dataset
MARS.probs <- predict(MARS.fit, newdata =biopsy.zs.test, type = "response")

## Calculate the confusion matrix 
InformationValue::confusionMatrix(testY, MARS.probs)
```

**Finding:**

11 variables were selected to construct the hinge function.



## Other classification algorithms

To provide a more comprehensive comparison, machine learning algorithms (classification and dimensionality reduction) may also provide better predictions.


### Linear discriminant analysis (LDA)

When the classification result is very certain, the estimated result of logistic regression may be unstable, the confidence interval may be wide, and the discriminant analysis has a stronger generalization ability.


```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
## Linear discriminant analysis
lda.fit <- lda(diagnosis ~ ., data = biopsy.zs.train)
lda.fit
 
## Performance on the test set
lda.probs <- predict(lda.fit, newdata = biopsy.zs.test)$posterior[, 2]

## Calculate the confusion matrix 
InformationValue::confusionMatrix(testY, lda.probs)
```

**Finding:**

Linear discriminant analysis assumes that the observations in Benign/Malignant obey a multivariate normal distribution, and the covariances between different categories are the same. The quadratic discriminant analysis still assumes that the observations follow a normal distribution, but assumes that each category has its own covariance. Here it is found that linear discriminant analysis performs better.



### Principal component analysis (PCA)  

PCA can reduce the data dimensions, and then logistic regression can be performed on the PCA variables.

For the model building process, follow the steps below:

1. Extract the principal components of 30 biomarkers based on the scree plot and decide the quantity to keep;
2. Rotate the principal components;
3. Calcualte scores for each components;
4. Use the score as an input variable for regression analysis, and use the test data to evaluate the effect of the model.


```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
## Extract the principal components 
## Rotation can reduce (or eliminate) the correlation between the principal components 
## and help explain the principal components.
pca <- principal(biopsy.zs.train[-1], nfactors = 8, rotate = "varimax")

## Find the point with small reduction rate. 
## The point means that when a new principal component is added to this point, 
## the explanation of variance does not increase too much.
plot(pca$values, type="b",main="Scree plot", ylab="Eigenvalues", xlab="Component")

## Calcualte scores  
pca.scores <- data.frame(pca$scores)
pca.scores$diagnosis <- biopsy.zs.train$diagnosis

## PCA Modeling using scores 
LR.PCA.fit <- glm(diagnosis ~ ., family = binomial, data = pca.scores)

## Model Results
summary(LR.PCA.fit)

## Performance on the test dataset
test.scores <- data.frame(predict(pca, biopsy.zs.test[-1]))
LR.PCA.probs <- predict(LR.PCA.fit, test.scores,type = "response")

## Calculate the confusion matrix 
InformationValue::confusionMatrix(testY, LR.PCA.probs)
```

**Finding:**

* PCA processes high-dimensional data (30 biomarkers) to low dimension data (PCA score), but inevitable loss is the interpretability of logistic regression.




### Support Vector Machine (SVM)

Support vector machine is a binary linear classifier, the following is modeled by linear SVM.


```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
## Find the optimal cost function with the smallest error rate of misclassification
set.seed(123)
svm.fit <- tune.svm(diagnosis ~ ., data = biopsy.zs.train, 
                    kernel = "linear", 
                    cost = c(0.001, 0.01, 0.1, 1, 5, 10))
summary(svm.fit)

## Performance on the test data set 
best.svm.fit <- svm.fit$best.model
svm.probs <- predict(best.svm.fit, newdata = biopsy.zs.test)

## Confusion matrix
table(svm.probs, testY)
```

**Finding:**

* It performs well on the test data set, but non-linear classification means giving up the interpretability of the model.
* Prediction is not based on probability, only confusion matrix can be displayed, and it is difficult to calculate AUC.


### K-Nearest Neighbors (KNN)

Like support vector machines, as a nonlinear technology, K-Nearest Neighbors no longer have to use linear combinations of features to define decision boundaries. The key point of using KNN is to select the suitable parameter k, where the grid is established to find the appropriate k for modeling.


```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}

## Use cross-validation to select parameter k
knn.grid <- expand.grid(.k = seq(2, 20, by = 1))
control = trainControl(method = "cv")
 
set.seed(123)
knn.train <- train(diagnosis ~ ., data = biopsy.zs.train, 
                   method = "knn", 
                   trControl = control, 
                   tuneGrid = knn.grid)
## Optimal k value
knn.train
## The Y-axis represents the percentage of observations that are misclassified by the kernel function
## k=3 reaches its peak
plot(knn.train)

## Apply to the test data set
knn.probs <- knn(biopsy.zs.train[, -1], biopsy.zs.test[, -1], 
                 biopsy.zs.train[, 1], k = 3,prob=TRUE)

## Confusion matrix
table(knn.probs, biopsy.zs.test$diagnosis) 
```


**Finding:**

* As the same as SVM, prediction is not based on probability, only Confusion matrix can be displayed.
* Interpretation of the model is difficult.


### Random Forest

Random forests are a powerful nonlinear mechanic learning method that can be used for classification or regression. During the construction of a random forest, random sampling of the training data set using Bagging, and the input feature of each tree is obtained by random sampling. This helps to reduce the impact of height-correlated predictive features (biomarkers).

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
## Build model
rf.fit <- randomForest(diagnosis ~ ., data = biopsy.zs.train)

## Confusion matrix in train dataset
rf.fit

## Variable Importance Plot
varImpPlot(rf.fit, main = "Variable importance using Random Forest")

## Show 9 variables important for classification
import <- rf.fit$importance
import[import>7,]

## Performance on the test dataset
rf.probs <- predict(rf.fit, newdata = biopsy.zs.test, type = "response")

## Confusion matrix in test dataset
table(rf.probs, testY)
```


**Finding:**

* The interpretation of the model is still difficult.
* prediction is not based on probability, and can only display confusion matrices.
* However, the positive aspect is that important biomarkers can be found in the plot.





# Model Analysis and Comparison

## Idea

Give confusion matrices (as follows), Misclassification and Accuracy can be used to evaluate and compare the above models.

$$
\begin{array}{c|cc} 
& \begin{array}{c}
\text { Predicted } \\
\text { Benign }
\end{array} & \begin{array}{c}
\text { Predicted } \\
\text { Malignant }
\end{array} \\
\hline \text { Actual Benign } & \begin{array}{c}
\text { True } \\
\text { Positive }
\end{array} & \begin{array}{c}
\text { False } \\
\text { Negative }
\end{array} \\
\text { Actual Malignant } & \begin{array}{c}
\text { False } \\
\text { Positive }
\end{array} & \begin{array}{c}
\text { True } \\
\text { Negative }
\end{array}
\end{array}
$$

Where:

* Misclassification is the overall error. Objective: minimize
* Accuracy shows how often is the classifier correct? Opposite of misclassification above.

In addition, a good binary classifier will have high precision and sensitivity. To capture this balance, the ROC curve is always used that plots the false positive rate along the x-axis and the true positive rate along the y-axis. A line that is diagonal from the lower-left corner to the upper right corner represents a random guess. The higher the line is in the upper left-hand corner, the better. The area under the curve (AUC) computes the area under this curve. 

While the ROC curve and corresponding AUC give an overall picture of the behavior of a diagnostic test across all cutoff values, there remains a practical need to determine the specific cutoff value that should be used for individuals requiring diagnosis of the benign and malignant breast.

Finally, for models with non-probabilistic classification predictions (SVM, KNN, RF), only misclassification and accuracy are provided. For other models, in addition to misclassification and accuracy, AUC and the optimal probability cutoff score will be computed. The above results will be performed on the test data set (will not be repeated on the training data set). Through comparison, the best model and the combination of potentially diagnostic variables for the distinguishment of benign vs. malignant tumors can be determined.



## Performance on the test dataset

The following models will be compared:

1. **"LR.Cor"**: Logistic regression (removing highly correlated items)
2. **"LR.Bestcor"**: Logistic regression (backward selection from the model removing highly correlated items)
3. **"Lasso"**: Lasso regression 
4. **"Elastic net"**: Elastic net regularization
5. **"MARS"**: Multiple adaptive regression spline
6. **"LDA"**: Linear discriminant analysis
7. **"LR.PCA"**: Logistic regression based on principal component analysis
8. **"SVM"**: Support Vector Machine
9. **"KNN"**: K-Nearest Neighbors
10. **"RF"**: Random Forest

```{r,echo = F,message = FALSE, error = FALSE, warning = FALSE}
Evaluation1 <- function(description,probs){
  
  ## Calculate the percentage misclassification error 
  ## for the given actuals and probaility scores
  Misclassification <- misClassError(y_test, probs)
  Accuracy <- 1-Misclassification
  
  ## Calculate the area uder ROC curve statistic for a given logit model. 
  AUC <- round(AUROC(y_test, probs),4)
  
  ## ## Compute the optimal probability cutoff score
  OptimalCutoff <- round(optimalCutoff(y_test, probs),3)
  
  return(data.frame(Model=description,
                    Misclassification=Misclassification,
                    Accuracy=Accuracy,
                    AUC=AUC,
                    OptimalCutoff=OptimalCutoff))
}

Evaluation2 <- function(description,probs){
  
  ## confusion matrix 
  ConMatrx <- table(probs, testY)
  
  ## Calculate the Accuracy
  Accuracy <- round(sum(diag(ConMatrx))/sum(ConMatrx),4)
  Misclassification <- 1-Accuracy
  
  return(data.frame(Model=description,
                    Misclassification=Misclassification,
                    Accuracy=Accuracy,
                    AUC="-",
                    OptimalCutoff="-"))
}

Modelname1 <- list("LR.Cor","LR.Bestcor","Lasso","Elastic net","MARS","LDA","LR.PCA")
Modelname2 <- list("SVM","KNN","RF")
Testprobs1 <- list(LR.cor.probs, LR.Bestcor.probs, lasso.probs,
                   enet.probs, MARS.probs, lda.probs, LR.PCA.probs)
Testprobs2 <- list(svm.probs, knn.probs, rf.probs)

results <- cbind(as.data.frame(mapply(Evaluation1, Modelname1, Testprobs1)),
                 as.data.frame(mapply(Evaluation2, Modelname2, Testprobs2)))
results <- as.data.frame(t(results))
rownames(results) <- NULL

kable(results,caption = "Model comparison results", format = "html") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```


## Summary 

The 3 models: **Lasso regression, Elastic net regularization, Support Vector Machine** have the smallest misclassification rate 0.0366 and the highest Accuracy 0.9634. For the models **Lasso regression, Elastic net regularization**, the Receiver Operating Characteristics (ROC) Curves are shown below and the optimal cutoff values are 0.42 and 0.34.

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
## ROC Curve of Lasso regression
plotROC(y_test, lasso.probs)

## ROC Curve of Elastic net regularization
plotROC(y_test, enet.probs)
```
 

## Conclusions

As the most commonly used linear model, the logistic model is the easiest to implement and easy to interpret, but the high correlation between variables and the non-linear relationship that is difficult to capture makes logistic regression unable to be applied well. The results above have the lowest accuracy (LR.Cor and LR.Bestcor). In contrast, regularization regression penalizes the coefficients and can deal with multicollinearity at the same time. It performs well in simulation research.

In addition, other machine learning algorithms are also potential candidates, such as SVM and PCA, which have an accuracy of more than 95%. However, on the one hand, the machine learning model is difficult to interpret, and on the other hand, it is more complicated when building the model (parameter selection, cross-validation, etc.), so in terms of practicality, penalized regression may be more advantageous.
 
All in all, this task is a very interesting and challenging task. In terms of content, the comparison of more than 10 models is comprehensive, but conversely, limited by time and experience, the depth and complexity of each model will be reduced. I spent three nights processing (around 16 hours). Among them, Cutoff analysis and AUC are rarely used in practice. But through this task, I can make a comprehensive comparison and summary of the classification algorithms that I am familiar with, which really benefited me a lot.


