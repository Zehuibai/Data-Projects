---
title: "Random Forests for Multiple Imputation of Missing Data in the IDEFICS study"
author: "Zehui Bai"
date: "22.01.2021"
output:
  beamer_presentation:
    citation_package: natbib
    colortheme: seahorse
    fig_caption: yes
    fonttheme: structurebold
    incremental: no
    keep_tex: no
    latex_engine: xelatex
    slide_level: 2
    theme: metropolis
    toc: no
  slidy_presentation:
    incremental: no
  ioslides_presentation:
    incremental: no
subtitle: Kolloquium zur Masterarbeit
colorlinks: yes
fontsize: 9pt
institute: FB3 Medical Biometry/Biostatistics
keywords: "Referee: Prof. Dr. Vanessa Didelez, Dr. Ronja Foraita  \nSupervisor: Janine
  Witte\n"
link-citations: yes
linkcolor: NavyBlue
classoption: dvipsnames,UTF8
section-titles: yes
biblio-style: apalike
---


```{r setup, include=FALSE}
options(digits = 3)
knitr::opts_chunk$set(
  comment = "#>",
  echo = FALSE,
  collapse = TRUE,
  out.width = "70%",
  fig.align = 'center',
  fig.asp = 0.618,  # 1 / phi
  fig.show = "hold"
)
```




# Introduction to Multiple Imputation

## Missing data

>* Problem: Missing data
>* Statistical analysis (biased)
>* Missing mechanisms (MCAR, MAR, MNAR)
>* Solution

  
## Single imputation
 
\begin{center}
\includegraphics[width=4.0in]{Fotos/Single_IM.png}
\end{center}

## Multiple imputation

* **Multiple imputation**
  
\begin{columns}[onlytextwidth,T]
  \begin{column}{.25\linewidth}
  \end{column}
  \begin{column}{.75\linewidth}
    \includegraphics[width=\linewidth]{Fotos/MI.png}
  \end{column}
\end{columns}



## Joint modeling and Fully Conditional Specification


>* 1.**Joint Modeling (JM)**: specifies a joint multivariate distribution for data

>* 2.**Fully Conditional Specification (MICE)**: specifies the conditional multivariate imputation model for variable



## Machine learning

1.**Classifications and regression trees (CART)** proposed by Breiman et al. (1984). 

\begin{center}
\includegraphics[width=3in]{Fotos/decision-tree.png}
\begin{figure}[!h]
\caption{Example of a regression tree}
\end{figure}
\end{center}

## Machine learning

2.**Random forest** (Breiman, 2001).





 
## Study motivation

\begin{center}
\includegraphics[width=4.2in]{Fotos/motivation.png}
\end{center}


 




# IDEFICS study

## IDEFICS study

\begin{center}
\includegraphics[width=1.9in]{Fotos/IDIFICS}
\end{center}


 

## Simualtion datasets  

\begin{center}
\includegraphics[width=3.7in]{Fotos/Variables_in_datasatz.png}
\end{center}









<!-- ---------------------------------------- -->

# Method


## Method

* MICE
    * **MICE PMM**
    * MICE RF
* Simulation Study Settings
* Analysis Methods
    * Linear regression
    * Conditional independence test
    




## Predictive mean matching

> * Hoc deck imputation
> * impute the missing value with the most similar object from the observed values (different criteria to judge similarity).

\begin{columns}[onlytextwidth,T]
  \begin{column}{.40\linewidth}
  \end{column}
  \begin{column}{.40\linewidth}
    \includegraphics[width=\linewidth]{Fotos/PMM1.png}
  \end{column}
\end{columns}






## Bayesian multiple imputation

**Regression imputation** and **Stochastic regression imputation** $\dot y=\hat\beta_0+X_\mathrm{mis}\hat\beta_1+\dot\epsilon$

\begin{columns}[onlytextwidth,T]
  \begin{column}{.50\linewidth}
    \includegraphics[width=\linewidth]{Fotos/regression1.png}
  \end{column}
  \begin{column}{.50\linewidth}
    \includegraphics[width=\linewidth]{Fotos/regression2.png}
  \end{column}
\end{columns}





 
## Bayesian multiple imputation

>* **Bayesian multiple imputation**
    * $\dot y =\dot\beta_0 + X_\mathrm{mis}\dot\beta_1+\dot\epsilon$    
    where $\hat\beta_0$, $\hat\beta_1$ and $\hat\sigma$ are random draws from their posterior distribution, given the data, e.g. $N(\beta_0,\sigma_{\beta_0}^2)$. 

<!-- predict + noise + parameters uncertainty. -->
<!-- https://stefvanbuuren.name/fimd/how-to-generate-multiple-imputations.html -->

\begin{columns}[onlytextwidth,T]
  \begin{column}{.40\linewidth}
  \end{column}
  \begin{column}{.60\linewidth}
    \includegraphics[width=\linewidth]{Fotos/regression3.png}
  \end{column}
\end{columns}








## MICE PMM Algorithm

\begin{columns}[onlytextwidth,T]
  \begin{column}{.40\linewidth}
     1. Coefficient $\hat\beta$ (method of least squares) is estimated using Bayesian linear regression given other variables.
     
     2. Regression coefficient $\dot\beta$ is randomly drawn from its posterior multivariate normal distribution $N(\hat\beta,\text{Var}(\hat\beta))$.    
     
     3. For each missing data of variable, calculate the distance $\dot d(i,j)=|X_i^\mathrm{obs}\hat\beta-X_j^\mathrm{mis}\dot\beta|$, where $i=1,\dots,n_1$ and $j=1,\dots,n_0$.
  \end{column}
  \begin{column}{.60\linewidth}
    \includegraphics[width=\linewidth]{Fotos/PMM2.png}
  \end{column}
\end{columns}


 


## MICE PMM Algorithm
 
\begin{columns}[onlytextwidth,T]
  \begin{column}{.40\linewidth}
    4. For each missing data $Y_j^\mathrm{mis}$, from $\dot d(i,j)$ create a set of $d$ donors, from $Y_\mathrm{obs}$ such that $\sum_d\dot\eta(i,j)$ is minimum.
    
    5. Randomly choose one donor.
    
    6. Repeat $m$ times.
  \end{column}
  \begin{column}{.60\linewidth}
    \includegraphics[width=\linewidth]{Fotos/PMM2.png}
  \end{column}
\end{columns}
 






## Method

* MICE
    * MICE PMM
    * **MICE RF**
* Simulation Study Settings
* Analysis Methods
    * Linear regression
    * Conditional independence test



 


 


## MICE RF (Algorithm 6)

\begin{center}
\includegraphics[width=4.3in]{Fotos/MICE_RF.png}
\end{center}



## The choose of Algorithm
 

|                 | **Algorithm 5**                | **Algorithm 6**                                           |
|-----------------|--------------------------------|-----------------------------------------------------------|
| **Proposer**    | Doove et al                    | Shah et al                                                |
| **Continuous**  | Randomly draw from predictions | Randomly draw from the normal distribution of predictions |
| **Categorical** | Randomly draw from predictions | Randomly draw from predictions                            |
| **Simulation**  | Large biases, low CIs coverage | Small bias, efficient estimates and narrower CIs          |








## Method

* MICE
    * MICE PMM
    * MICE RF
* **Simulation Study Settings**
* Analysis Methods
    * Linear regression
    * Conditional independence test
 




## Simulation Study Setting 
 
 
\begin{center}
\includegraphics[width=4.3in]{Fotos/Settings.png}
\end{center}






## Method

* MICE
    * MICE PMM
    * MICE RF
* Simulation Study Settings
* **Analysis Methods**


## Analysis Methods

\begin{center}
\includegraphics[width=4.5in]{Fotos/Analysis.png}
\end{center}
 
 

 
 
 


<!-- ---------------------------------------- -->

# Results

## Results using linear regression: Setting 1

\begin{center}
\includegraphics[width=3.5in]{Fotos/Results_1.png}
\end{center}

>* The performance of the parametric MICE PMM was better than MICE RF. 
>* MICE PMM had smaller z-scores.
>* Performance using MICE RF based on 100 trees was better than 10 trees, smaller z-score.



## Results using linear regression: Setting 1

\begin{center}
\includegraphics[width=3.5in]{Fotos/Results_2.png}
\end{center}

>* MICE RF: had excellent performance in the categorical variable "ISCED" and interaction term "age:sex".
>* MICE RF: for continuous variables and interaction term "age:bmi", the coverage was less than 90%.
>* MICE PMM: had higher CI coverages. 



## Results using linear regression: Setting 1 (BMI, Age)

\begin{center}
\includegraphics[width=4.0in]{Fotos/Density_Full.jpg}
\end{center}

>* MICE RF assumed conditional normality and constant variance from the random forest regression.






## Results using linear regression: Setting 2

\begin{center}
\includegraphics[width=3.8in]{Fotos/Results_3.png}
\end{center}

>* Sample size $\uparrow$ z-scores $\uparrow$.
>* Missing ratio $\uparrow$ z-scores $\uparrow$ (MICE PMM smaller).



## Results using linear regression: Setting 3


\begin{center}
\includegraphics[width=3.8in]{Fotos/Results_5.png}
\end{center}

>* The performance of MICE PMM (interactions) improved with smaller z-score.


## Results using linear regression: Setting 3


\begin{center}
\includegraphics[width=3.8in]{Fotos/Results_6.png}
\end{center}

>* The performance of MICE PMM (interactions) improved with increased CI coverage. 





## Results using conditional independence test: Setting 1

 
\begin{center}
\includegraphics[width=3.0in]{Fotos/CIT_1.png}
\end{center}

>* The type 1 errors of MICE RFs were smaller (100 trees)



## Results using conditional independence test: Setting 2


\begin{columns}[onlytextwidth,T]
  \begin{column}{.65\linewidth}
    \includegraphics[width=\linewidth]{Fotos/CIT_2.png}
  \end{column}
  \begin{column}{.30\linewidth}
   1. sample size $\uparrow$ or missing ratio $\uparrow$ $\rightarrow$ the actual type 1 error $\uparrow$


   2. MICE RF was slightly better than MICE PMM 
  \end{column}
\end{columns}


 


## Results using conditional independence test: Setting 3


\begin{center}
\includegraphics[width=3.0in]{Fotos/CIT_3.png}
\end{center}

>* MICE PMM 1 exceeded the nominal level of 5% $\rightarrow$ not conservative.


## Results using conditional independence test: Power


\begin{columns}[onlytextwidth,T]
  \begin{column}{.65\linewidth}
    \includegraphics[width=\linewidth]{Fotos/CIT_4.png}
  \end{column}
  \begin{column}{.30\linewidth}
   n=30: power was very sensitive to the missing ratio (MICE PMM better) 
   
   
   n=50, 70, $\uparrow$: MICE PMM slight advantage
  \end{column}
\end{columns}

 


<!-- ---------------------------------------- -->

# Conclusion und Prospects
 

 

## MICE PMM vs. MICE RF

|                | **MICE PMM**                 | **MICE RF**                            |
|----------------|------------------------------|----------------------------------------|
| **Strengths**  | Suit for Mixed data          | Suit for Mixed data                    |
|                | Match the distribution       | Accommodate interactions               |
|                | Imputation within the range  | Adapt compatibility                    |
|                | Shorter program running time |                                        |
| **Weaknesses** | Incompatible                 | Assumption of normality                |
|                | Interactions (complex data)  | Meaningless imputation                 |
|                | Insufficient donors          | Calculation expensively                |

 
 
 




## Weaknesses and Prospects

|   | **Weaknesses**            | **Prospects**                                |
|---|---------------------------|----------------------------------------------|
| 1 | Variable selection        | $\star$ Derived variable                           |
|   |                           | $\star$ Observed partial correlation is close to 0 |
| 2 | Other auxiliary variables | $\star$ Construct random forests                     |
| 3 | Passive imputation        | $\star$ Other methods (JAV)                          |
| 4 | MICE RF Algorithm         | $\star$ (Approximately) normally distributed       |
|   |                           | $\star$ Algorithm 5                                |


 

 

<!-- ---------------------------------------- -->




## Vielen Dank


\begin{center}
\includegraphics[width=2.0in]{Fotos/Thanks.jpg}
\end{center}






<!-- ---------------------------------------- -->

# Reference



Seaman SR, Bartlett JW, White IR. (2012). Multiple imputation of missing covariates with non-linear effects and interactions: an evaluation of statistical methods. BMC Med Res Methodol. 12:46. Published 2012 Apr 10.

Van Buuren, S. (2012). Flexible Imputation of Missing Data. Chapman and Hall/CRC Press.

Cole TJ, Lobstein T. (2012). Extended international (IOTF) body mass index cut-offs for thinness, overweight and obesity. Pediatr Obes. 7(4):284-94.

Shah AD, Bartlett JW, Carpenter J, Nicholas O, Hemingway H. (2014). Comparison of random forest and parametric imputation models for imputing missing data using MICE: a CALIBER study. Am J Epidemiol, 179(6):764–774.

L.L. Doove, S. Van Buuren, E. Dusseldorp. (2014). Recursive partitioning for missing data imputation in the presence of interaction effects. Computational Statistics & Data Analysis, Volume 72, Pages 92-104, ISSN 0167-9473.

##

Bartlett, J.W. et al., (2015). Multiple imputation of covariates by fully conditional specification: Accommodating the substantive model. Statistical methods in medical research, 24(4), pp.462–487.

Ahrens W, Siani A, Adan R, et al. (2017). Cohort Profile: The transition from childhood to adolescence in European children-how I.Family extends the IDEFICS cohort. Int J Epidemiol. 46(5):1394‐1395j. doi:10.1093/ije/dyw317

Stekhoven DJ. (2012). missForest: Nonparametric Missing Value Imputation using Random Forest. Vienna, Austria: Comprehensive R Archive Network. (Accessed May 15, 2020)


